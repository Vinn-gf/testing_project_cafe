# API Evaluate (leave–one–out)
@app.route("/api/evaluate")
def api_evaluate():
    mat, sim, knn = build_cf_model()
    hits, mrrs = [], []

    for uid in fetch_all_user_ids():
        seq = [v["id_cafe"] for v in fetch_visited(uid) if isinstance(v, dict)]
        if len(seq) < 2: continue
        test, hist = seq[-1], seq[:-1]

        cf_raw = rec_ubcf_scores(uid, mat, sim, knn)
        vf_raw = rec_visited_freq(uid)
        co_raw = rec_menu_cooccur(uid)

        max_cf = max(cf_raw.values()) if cf_raw else 1.0
        max_vf = max(vf_raw.values()) if vf_raw else 1.0
        max_co = max(co_raw.values()) if co_raw else 1.0

        pool = (set(cf_raw) | set(vf_raw) | set(co_raw)) - set(hist)
        α, β, γ = 0.6, 0.2, 0.2
        scored = []
        for cid in pool:
            cf_n = cf_raw.get(cid, 0) / max_cf
            vf_n = vf_raw.get(cid, 0) / max_vf
            co_n = co_raw.get(cid, 0) / max_co
            scored.append((cid, α*cf_n + β*vf_n + γ*co_n))

        ranked = [cid for cid, _ in sorted(scored, key=lambda x: -x[1])]
        hits.append(1 if test in ranked else 0)
        mrrs.append(1.0/(ranked.index(test)+1) if test in ranked else 0.0)

    return jsonify({
        "HitRate": round(np.mean(hits),4),
        "MRR":      round(np.mean(mrrs),4)
    })